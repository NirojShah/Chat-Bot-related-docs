from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = Flask(__name__)

# Load the GPT-2 model and tokenizer
def load_model(model_dir="./models/gpt2-small"):
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForCausalLM.from_pretrained(model_dir)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return model, tokenizer, device

model_dir = "./models/gpt2-small"
model, tokenizer, device = load_model(model_dir)

# Load extracted PDF text
with open("pdf_text.txt", "r") as file:
    pdf_text = file.read()

@app.route('/chatbot', methods=['POST'])
def chatbot():
    user_input = request.json.get('message')
    
    # Check if the user input is related to the PDF content
    if is_related_to_pdf(user_input):
        response = generate_response(user_input, pdf_text, model, tokenizer, device)
    else:
        # Provide a generic response if the question is not related to the PDF content
        response = "I'm sorry, I can only answer questions related to the content of the PDF."
    
    return jsonify({"response": response})

def is_related_to_pdf(user_input):
    # You can implement your logic here to determine if the user input is related to the PDF content
    # For simplicity, let's assume any question containing the word "PDF" is considered related
    return "PDF" in user_input.upper()

def generate_response(user_input, context, model, tokenizer, device, max_length=150):
    prompt = f"Context: {context}\n\nUser: {user_input}\nBot:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[1].strip()

if __name__ == '__main__':
    app.run(debug=True)
